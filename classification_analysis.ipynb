{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Analysis\n",
    "\n",
    "## Midterm and Final Project: Classification Analysis\n",
    "\n",
    "This notebook provides a comprehensive framework for performing classification analysis on various datasets. It includes data exploration, preprocessing, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    ")\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load your dataset here. This example uses a placeholder - replace with your actual data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load data from CSV\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# For demonstration, we'll use sklearn's built-in dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df['target_names'] = df['target'].map(dict(enumerate(data.target_names)))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Understand the dataset through statistical summaries and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['target_names'].value_counts().plot(kind='bar', ax=ax[0], color='skyblue', edgecolor='black')\n",
    "ax[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax[0].set_xlabel('Class', fontsize=12)\n",
    "ax[0].set_ylabel('Count', fontsize=12)\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "df['target_names'].value_counts().plot(kind='pie', ax=ax[1], autopct='%1.1f%%', startangle=90)\n",
    "ax[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop('target')\n",
    "n_cols = len(numeric_cols)\n",
    "n_rows = (n_cols + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(14, 4 * n_rows))\n",
    "axes = axes.flatten() if n_cols > 1 else [axes]\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].hist(df[col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide empty subplots if odd number of features\n",
    "for idx in range(n_cols, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(['target', 'target_names'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\\n{y_train.value_counts().sort_index()}\")\n",
    "print(f\"\\nTesting set class distribution:\\n{y_test.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nTraining set mean (after scaling): {X_train_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"Training set std (after scaling): {X_train_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train multiple classification models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"Training {len(classifiers)} different classifiers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': clf,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_score': cv_mean,\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Score: {cv_mean:.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Comparison\n",
    "\n",
    "Compare the performance of different models using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
    "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
    "    'F1-Score': [results[m]['f1_score'] for m in results.keys()],\n",
    "    'CV Score': [results[m]['cv_score'] for m in results.keys()],\n",
    "    'CV Std': [results[m]['cv_std'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    data = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    ax.barh(data['Model'], data[metric], color=color, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(data[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation scores comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models = list(results.keys())\n",
    "cv_means = [results[m]['cv_score'] for m in models]\n",
    "cv_stds = [results[m]['cv_std'] for m in models]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "ax.bar(x_pos, cv_means, yerr=cv_stds, capsize=5, color='teal', \n",
    "       edgecolor='black', alpha=0.7, error_kw={'linewidth': 2})\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.set_ylabel('Cross-Validation Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Cross-Validation Scores with Standard Deviation', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (mean, std) in enumerate(zip(cv_means, cv_stds)):\n",
    "    ax.text(i, mean + std + 0.02, f'{mean:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model analysis\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True, \n",
    "            cbar_kws={'shrink': 0.8}, linewidths=1, linecolor='black')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for the classification task (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "tree_based_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "available_tree_models = [m for m in tree_based_models if m in results]\n",
    "\n",
    "if available_tree_models:\n",
    "    fig, axes = plt.subplots(1, len(available_tree_models), \n",
    "                            figsize=(7 * len(available_tree_models), 6))\n",
    "    \n",
    "    if len(available_tree_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, model_name in enumerate(available_tree_models):\n",
    "        model = results[model_name]['model']\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.bar(range(len(importances)), importances[indices], \n",
    "               color='forestgreen', edgecolor='black', alpha=0.7)\n",
    "        ax.set_title(f'Feature Importance - {model_name}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Feature Index', fontsize=10)\n",
    "        ax.set_ylabel('Importance', fontsize=10)\n",
    "        ax.set_xticks(range(len(importances)))\n",
    "        ax.set_xticklabels([X.columns[i] for i in indices], rotation=45, ha='right')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No tree-based models available for feature importance analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Persistence\n",
    "\n",
    "Save the best model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "scaler_filename = 'scaler.pkl'\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"Best model saved as: {model_filename}\")\n",
    "print(f\"Scaler saved as: {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Next Steps\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "This notebook demonstrated a comprehensive classification analysis workflow including:\n",
    "\n",
    "1. **Data Exploration**: Understanding the dataset structure, distributions, and relationships\n",
    "2. **Data Preprocessing**: Preparing data for machine learning (scaling, splitting)\n",
    "3. **Model Training**: Training multiple classification algorithms\n",
    "4. **Model Evaluation**: Comparing models using various metrics (accuracy, precision, recall, F1-score)\n",
    "5. **Feature Analysis**: Understanding feature importance (for applicable models)\n",
    "6. **Model Persistence**: Saving the best model for deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Hyperparameter Tuning**: Use GridSearchCV or RandomizedSearchCV to optimize model parameters\n",
    "- **Feature Engineering**: Create new features or transform existing ones\n",
    "- **Handling Imbalanced Data**: Apply techniques like SMOTE if classes are imbalanced\n",
    "- **Ensemble Methods**: Combine multiple models for better performance\n",
    "- **Cross-Validation**: Implement more robust cross-validation strategies\n",
    "- **Deploy Model**: Create API endpoint or application for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Making predictions with the saved model\n",
    "# Load the model\n",
    "# with open(model_filename, 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n",
    "\n",
    "# with open(scaler_filename, 'rb') as f:\n",
    "#     loaded_scaler = pickle.load(f)\n",
    "\n",
    "# Make predictions on new data\n",
    "# new_data_scaled = loaded_scaler.transform(new_data)\n",
    "# predictions = loaded_model.predict(new_data_scaled)\n",
    "\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
